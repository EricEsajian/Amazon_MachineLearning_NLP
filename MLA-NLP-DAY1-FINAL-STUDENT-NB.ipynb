{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Day 1: Logistic Regression Model for the Product Safety Dataset\n",
    "\n",
    "For the final project, build a Logistic Regression model to predict the __human_tag__ field of the dataset. You will submit your predictions to the Leaderboard competition here: https://leaderboard.corp.amazon.com/tasks/352\n",
    "\n",
    "You can use the __MLA-NLP-DAY1-LOGISTIC-REGR-NB__ notebook as yor starting code. Train and test your model with the corresponding datasets provided here. We are using F1 score to rank submissions. Sklearn provides the [__f1_score():__](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) function if you want to see how your model works on your training or validation set.\n",
    "\n",
    "You can follow these steps:\n",
    "1. Read training-test data (Given)\n",
    "2. Train a Logistic Regression model (Implement)\n",
    "3. Make predictions on your test dataset (Given)\n",
    "4. Write your test predictions to a CSV file (Given)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade dependencies\n",
    "! pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk, re\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from os import path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import BCELoss\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading the dataset (Given)\n",
    "\n",
    "We will use the __pandas__ library to read our dataset. Let's first run the following credential cell and then download the files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Training data:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../../data/final_project/training.csv', encoding='utf-8', header=0)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Test data:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../../data/final_project/test.csv', encoding='utf-8', header=0)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train a Logistic Regression Model (Implement)\n",
    "Here, we apply pre-processing and vectorization operations and train the model. You can use the __MLA-NLP-DAY1-LOGISTIC-REGR-NB__ notebook as yor starting code. We are using the F1 score in the competition. In sklearn, you can use the [__f1_score():__](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) function to see your F1 score on your training or validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Split Training data into training and validation and process text field (given)\n",
    "Here, we give you the code to split your dataset into training and validation sets and then process their text fields. You can start with this. Later, you can experiment with some changes here such as changing the size of your bag of words features (max_len) or trying different preprocessing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first process the text data\n",
    "\n",
    "print(\"Fixing missing values...\")\n",
    "# Fixing the missing values\n",
    "train_df[\"text\"].fillna(\"\", inplace=True)\n",
    "\n",
    "print(\"Splitting data into training and validation...\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_df[[\"text\"]],\n",
    "    train_df[\"human_tag\"].values,\n",
    "    test_size=0.10,\n",
    "    shuffle=True,\n",
    "    random_state=324,\n",
    ")\n",
    "\n",
    "# Stop words removal and stemming\n",
    "# Let's get a list of stop words from the NLTK library\n",
    "stop = stopwords.words(\"english\")\n",
    "\n",
    "# These words are important for our problem. We don't want to remove them.\n",
    "excluding = [\n",
    "    \"against\",\n",
    "    \"not\",\n",
    "    \"don\",\n",
    "    \"don't\",\n",
    "    \"ain\",\n",
    "    \"aren\",\n",
    "    \"aren't\",\n",
    "    \"couldn\",\n",
    "    \"couldn't\",\n",
    "    \"didn\",\n",
    "    \"didn't\",\n",
    "    \"doesn\",\n",
    "    \"doesn't\",\n",
    "    \"hadn\",\n",
    "    \"hadn't\",\n",
    "    \"hasn\",\n",
    "    \"hasn't\",\n",
    "    \"haven\",\n",
    "    \"haven't\",\n",
    "    \"isn\",\n",
    "    \"isn't\",\n",
    "    \"mightn\",\n",
    "    \"mightn't\",\n",
    "    \"mustn\",\n",
    "    \"mustn't\",\n",
    "    \"needn\",\n",
    "    \"needn't\",\n",
    "    \"shouldn\",\n",
    "    \"shouldn't\",\n",
    "    \"wasn\",\n",
    "    \"wasn't\",\n",
    "    \"weren\",\n",
    "    \"weren't\",\n",
    "    \"won\",\n",
    "    \"won't\",\n",
    "    \"wouldn\",\n",
    "    \"wouldn't\",\n",
    "]\n",
    "\n",
    "# New stop word list\n",
    "stop_words = [word for word in stop if word not in excluding]\n",
    "\n",
    "snow = SnowballStemmer(\"english\")\n",
    "\n",
    "def process_text(texts):\n",
    "    final_text_list = []\n",
    "    for sent in texts:\n",
    "\n",
    "        # Check if the sentence is a missing value\n",
    "        if isinstance(sent, str) == False:\n",
    "            sent = \"\"\n",
    "\n",
    "        filtered_sentence = []\n",
    "        \n",
    "        # Lowercase\n",
    "        sent = sent.lower()\n",
    "        # Remove leading/trailing whitespace\n",
    "        sent = sent.strip()\n",
    "        # Remove extra space and tabs\n",
    "        sent = re.sub(\"\\s+\", \" \", sent)\n",
    "        # Remove HTML tags/markups:\n",
    "        sent = re.compile(\"<.*?>\").sub(\"\", sent)\n",
    "\n",
    "        for w in word_tokenize(sent):\n",
    "            # We are applying some custom filtering here, feel free to try different things\n",
    "            # Check if it is not numeric and its length>2 and not in stop words\n",
    "            if (not w.isnumeric()) and (len(w) > 2) and (w not in stop_words):\n",
    "                # Stem and add to filtered list\n",
    "                filtered_sentence.append(snow.stem(w))\n",
    "        final_string = \" \".join(filtered_sentence)  # final string of cleaned words\n",
    "\n",
    "        final_text_list.append(final_string)\n",
    "\n",
    "    return final_text_list\n",
    "\n",
    "print(\"Processing the text fields...\")\n",
    "X_train[\"text\"] = process_text(X_train[\"text\"].tolist())\n",
    "X_val[\"text\"] = process_text(X_val[\"text\"].tolist())\n",
    "\n",
    "# Use TD-IDF to vectorize to vectors of len 750.\n",
    "tf_idf_vectorizer = TfidfVectorizer(max_features=750)\n",
    "\n",
    "# Fit the vectorizer to training data\n",
    "# Don't use the fit() on validation or test datasets\n",
    "tf_idf_vectorizer.fit(X_train[\"text\"].values)\n",
    "\n",
    "print(\"Transforming the text fields (Bag of Words)...\")\n",
    "# Transform text fields\n",
    "X_train = tf_idf_vectorizer.transform(X_train[\"text\"].values).toarray()\n",
    "X_val = tf_idf_vectorizer.transform(X_val[\"text\"].values).toarray()\n",
    "\n",
    "print(\"Shapes of features: Training and Validation\")\n",
    "print(X_train.shape, X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Train your neural network (implement)\n",
    "Train your neural network using the training data (X_train) and validation data (X_val) from above. Don't forget to create the data loaders etc that you need here. You can simply use the code from your logistic regression notebook (__MLA-NLP-DAY1-LOGISTIC-REGR-NB__) and try different hyperparameters such batch size and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Make predictions on your test dataset (given)\n",
    "\n",
    "Let's make predictions on the test dataset. We apply the same processes as we did earlier on the train-val datasets. \n",
    "\n",
    "We do the following. You don't need to change this part.\n",
    "1. Fill-in missing values: -> fillna()\n",
    "2. Clean and normalize text: -> process_text()\n",
    "3. Vectorize with your tf_idf_vectorizer. Use the transform() function: -> tf_idf_vectorizer.transform().toarray()\n",
    "4. Convert to Torch tensor: -> torch.tensor(output_of_transform, dtype=torch.float32).to(device)\n",
    "5. Get predictions: -> net(torch_test_data)\n",
    "6. Round up to 1 or down to 0: -> np.rint(test_predictions.detach().cpu().numpy())\n",
    "\n",
    "You will save your predictions (__test_predictions__ variable) to a CSV file in section 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing the missing values\n",
    "print(\"Fixing the missing values...\")\n",
    "test_df[\"text\"].fillna(\"\", inplace=True)\n",
    "print(\"Processing the text field...\")\n",
    "test_df[\"text\"] = process_text(test_df[\"text\"].tolist())\n",
    "print(\"Transforming the text field...\")\n",
    "X_test = tf_idf_vectorizer.transform(test_df[\"text\"].values).toarray()\n",
    "print(\"Converting it to Torch tensor...\")\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "print(\"Making the test predictions...\")\n",
    "test_predictions = net(X_test)\n",
    "test_predictions = np.rint(test_predictions.detach().cpu().numpy())\n",
    "test_predictions = np.squeeze(test_predictions)\n",
    "print(\"Here is the test predictions:\", test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Write your predictions to a CSV file and submit to the contest\n",
    "You can use the following code to write your test predictions to a CSV file. Then upload your file to https://mlu.corp.amazon.com/contests/redirect/53 Look at __\"data/final_project\"__ folder to find your file: project_day1_result.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "result_df = pd.DataFrame()\n",
    "result_df[\"ID\"] = test_df[\"ID\"]\n",
    "result_df[\"human_tag\"] = test_predictions\n",
    " \n",
    "result_df.to_csv(\"../../data/final_project/project_day1_result.csv\", encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
