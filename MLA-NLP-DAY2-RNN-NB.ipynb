{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"0\">Machine Learning Accelerator - Natural Language Processing - Lecture 2</a>\n",
    "\n",
    "## Recurrent Neural Networks (RNNs) for the Product Review Problem - Classify Product Reviews as Positive or Not\n",
    "\n",
    "In this exercise, we will learn how to use Recurrent Neural Networks. \n",
    "\n",
    "We will follow these steps:\n",
    "1. <a href=\"#1\">Reading the dataset</a>\n",
    "2. <a href=\"#2\">Exploratory data analysis</a>\n",
    "3. <a href=\"#3\">Train-validation dataset split</a>\n",
    "4. <a href=\"#4\">Text Transformation</a>\n",
    "5. <a href=\"#5\">Generating data batch and iterator</a>\n",
    "6. <a href=\"#6\">Using pre-trained GloVe Word Embeddings</a>\n",
    "7. <a href=\"#7\">Setting Hyperparameters and Bulding the Network</a>\n",
    "8. <a href=\"#8\">Training the Network</a>\n",
    "9. <a href=\"#9\">Improvement ideas</a>\n",
    "\n",
    "Overall dataset schema:\n",
    "* __reviewText:__ Text of the review\n",
    "* __summary:__ Summary of the review\n",
    "* __verified:__ Whether the purchase was verified (True or False)\n",
    "* __time:__ UNIX timestamp for the review\n",
    "* __log_votes:__ Logarithm-adjusted votes log(1+votes)\n",
    "* __isPositive:__ Whether the review is positive or negative (1 or 0)\n",
    "\n",
    "__Important note:__ One big distinction betweeen the regular neural networks and RNNs is that RNNs work with sequential data. In our case, RNNs will help us with the text field. If we also want to consider other fields such as time, log_votes, verified, etc., we need to use the regular neural networks with the RNN network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:02:47.162268Z",
     "start_time": "2021-01-09T05:02:47.160085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==1.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ../../requirements.txt (line 1)) (1.8.1)\n",
      "Requirement already satisfied: torchtext==0.9.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ../../requirements.txt (line 2)) (0.9.1)\n",
      "Requirement already satisfied: nltk==3.6.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ../../requirements.txt (line 3)) (3.6.2)\n",
      "Requirement already satisfied: pandas==1.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ../../requirements.txt (line 4)) (1.1.5)\n",
      "Requirement already satisfied: scikit-learn==0.24.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ../../requirements.txt (line 5)) (0.24.1)\n",
      "Requirement already satisfied: numpy==1.19.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ../../requirements.txt (line 6)) (1.19.5)\n",
      "Requirement already satisfied: trax==1.3.7 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ../../requirements.txt (line 7)) (1.3.7)\n",
      "Requirement already satisfied: transformers==4.5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ../../requirements.txt (line 8)) (4.5.1)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch==1.8.1->-r ../../requirements.txt (line 1)) (0.8)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch==1.8.1->-r ../../requirements.txt (line 1)) (3.10.0.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchtext==0.9.1->-r ../../requirements.txt (line 2)) (4.61.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchtext==0.9.1->-r ../../requirements.txt (line 2)) (2.26.0)\n",
      "Requirement already satisfied: regex in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from nltk==3.6.2->-r ../../requirements.txt (line 3)) (2021.4.4)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from nltk==3.6.2->-r ../../requirements.txt (line 3)) (1.0.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from nltk==3.6.2->-r ../../requirements.txt (line 3)) (8.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pandas==1.1.5->-r ../../requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pandas==1.1.5->-r ../../requirements.txt (line 4)) (2021.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-learn==0.24.1->-r ../../requirements.txt (line 5)) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-learn==0.24.1->-r ../../requirements.txt (line 5)) (1.5.3)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from trax==1.3.7->-r ../../requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-datasets in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from trax==1.3.7->-r ../../requirements.txt (line 7)) (4.4.0)\n",
      "Requirement already satisfied: tensorflow-text in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from trax==1.3.7->-r ../../requirements.txt (line 7)) (2.6.0)\n",
      "Requirement already satisfied: jaxlib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from trax==1.3.7->-r ../../requirements.txt (line 7)) (0.1.69)\n",
      "Requirement already satisfied: jax in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from trax==1.3.7->-r ../../requirements.txt (line 7)) (0.2.17)\n",
      "Requirement already satisfied: gin-config in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from trax==1.3.7->-r ../../requirements.txt (line 7)) (0.5.0)\n",
      "Requirement already satisfied: gym in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from trax==1.3.7->-r ../../requirements.txt (line 7)) (0.21.0)\n",
      "Requirement already satisfied: t5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from trax==1.3.7->-r ../../requirements.txt (line 7)) (0.9.3)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from trax==1.3.7->-r ../../requirements.txt (line 7)) (5.8.0)\n",
      "Requirement already satisfied: funcsigs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from trax==1.3.7->-r ../../requirements.txt (line 7)) (1.0.2)\n",
      "Requirement already satisfied: absl-py in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from trax==1.3.7->-r ../../requirements.txt (line 7)) (0.12.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.5.1->-r ../../requirements.txt (line 8)) (4.8.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.5.1->-r ../../requirements.txt (line 8)) (3.0.12)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.5.1->-r ../../requirements.txt (line 8)) (21.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.5.1->-r ../../requirements.txt (line 8)) (0.10.3)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.5.1->-r ../../requirements.txt (line 8)) (0.0.46)\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/six-1.16.0.dist-info/METADATA'\n",
      "\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Upgrade dependencies\n",
    "!pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:02:48.342987Z",
     "start_time": "2021-01-09T05:02:47.164823Z"
    }
   },
   "outputs": [],
   "source": [
    "import re, time\n",
    "import numpy as np\n",
    "import torch, torchtext\n",
    "import boto3\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from os import path\n",
    "from collections import Counter\n",
    "from torch import nn, optim\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import Vocab\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from torchtext.vocab import GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. <a name=\"1\">Reading the dataset</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the dataset below and look at the first five rows in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:02:48.995226Z",
     "start_time": "2021-01-09T05:02:48.344888Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>verified</th>\n",
       "      <th>time</th>\n",
       "      <th>log_votes</th>\n",
       "      <th>isPositive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65886</td>\n",
       "      <td>Purchased as a quick fix for a needed Server 2...</td>\n",
       "      <td>Easy install, seamless migration</td>\n",
       "      <td>True</td>\n",
       "      <td>1458864000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19822</td>\n",
       "      <td>So far so good. Installation was simple. And r...</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>True</td>\n",
       "      <td>1417478400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14558</td>\n",
       "      <td>Microsoft keeps making Visual Studio better. I...</td>\n",
       "      <td>This is the best development tool I've ever used.</td>\n",
       "      <td>False</td>\n",
       "      <td>1252886400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39708</td>\n",
       "      <td>Very good product.</td>\n",
       "      <td>Very good product.</td>\n",
       "      <td>True</td>\n",
       "      <td>1458604800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8015</td>\n",
       "      <td>So very different from my last version and I a...</td>\n",
       "      <td>... from my last version and I am having a gre...</td>\n",
       "      <td>True</td>\n",
       "      <td>1454716800</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                         reviewText  \\\n",
       "0  65886  Purchased as a quick fix for a needed Server 2...   \n",
       "1  19822  So far so good. Installation was simple. And r...   \n",
       "2  14558  Microsoft keeps making Visual Studio better. I...   \n",
       "3  39708                                 Very good product.   \n",
       "4   8015  So very different from my last version and I a...   \n",
       "\n",
       "                                             summary  verified        time  \\\n",
       "0                   Easy install, seamless migration      True  1458864000   \n",
       "1                                         Five Stars      True  1417478400   \n",
       "2  This is the best development tool I've ever used.     False  1252886400   \n",
       "3                                 Very good product.      True  1458604800   \n",
       "4  ... from my last version and I am having a gre...      True  1454716800   \n",
       "\n",
       "   log_votes  isPositive  \n",
       "0   0.000000         1.0  \n",
       "1   0.000000         1.0  \n",
       "2   0.000000         1.0  \n",
       "3   0.000000         1.0  \n",
       "4   2.197225         0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../data/examples/NLP-REVIEW-DATA-CLASSIFICATION-TRAINING.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <a name=\"2\">Exploratory Data Analysis</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the range and distribution of the target column `isPositive`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:02:49.024615Z",
     "start_time": "2021-01-09T05:02:49.017492Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    34954\n",
       "0.0    21046\n",
       "Name: isPositive, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"isPositive\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the number of missing values for each columm below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:02:49.040120Z",
     "start_time": "2021-01-09T05:02:49.026288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID             0\n",
      "reviewText    10\n",
      "summary       12\n",
      "verified       0\n",
      "time           0\n",
      "log_votes      0\n",
      "isPositive     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have missing values in our text fields. We will use the __reviewText__ field, so we fill-in the missing values in it iwth the empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"reviewText\"] = df[\"reviewText\"].fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. <a name=\"3\">Train-validation split</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the dataset into training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:02:49.098503Z",
     "start_time": "2021-01-09T05:02:49.041948Z"
    }
   },
   "outputs": [],
   "source": [
    "# This separates 10% of the entire dataset into validation dataset.\n",
    "train_text, val_text, train_label, val_label = train_test_split(\n",
    "    df[\"reviewText\"].tolist(),\n",
    "    df[\"isPositive\"].tolist(),\n",
    "    test_size=0.10,\n",
    "    shuffle=True,\n",
    "    random_state=324,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. <a name=\"4\">Text Transformation</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We will apply the following processes here:\n",
    "1. Creating a vocabulary\n",
    "2. Text transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. Creating a vocabulary:__ \n",
    "\n",
    "We will create a vocabulary with the tokens from the text data. We use a simple english tokenizer and use these tokens to create our vocabulary. In this vocabulary, tokens will map to unique ids, such as \"car\"->32, \"house\"->651, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "counter = Counter()\n",
    "for line in train_text:\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = Vocab(counter, min_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'home' -> 211\n",
      "'wash' -> 10241\n",
      "'fhshbasdhb' -> 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"'home' -> {vocab['home']}\")\n",
    "print(f\"'wash' -> {vocab['wash']}\")\n",
    "# unknown word (assume from test set)\n",
    "print(f\"'fhshbasdhb' -> {vocab['fhshbasdhb']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Text transformation:__ \n",
    "\n",
    "We will use the vocabulary and map tokens in the text to unique ids of the tokens. For example: `[\"this\", \"is\", \"a\", \"sentence\"] -> [14, 12, 9, 2066]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a mapper to transform our text data\n",
    "text_transform_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some text before and after transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before transform:\tHappy to own it.\n",
      "After transform:\t[321, 6, 237, 8, 2]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before transform:\\t{train_text[37]}\")\n",
    "print(f\"After transform:\\t{text_transform_pipeline(train_text[37])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function for this. In this function, we transform and pad (if necessary) our text data. We cut the series of words at the point where it reaches a certain lenght (we used `max_len=50` here). If the text is shorter than max_len, we `pad zeros` to the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformText(text_list, max_len):\n",
    "    # Transform the text\n",
    "    transformed_data = [text_transform_pipeline(text)[:max_len] for text in text_list]\n",
    "\n",
    "    # Pad zeros if the text is shoter than max_len\n",
    "    for data in transformed_data:\n",
    "        data[len(data) : max_len] = np.zeros(max_len - len(data))\n",
    "\n",
    "    return torch.tensor(transformed_data, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ['Horrible. One day of frustration and I am back to the PC version.', \"Didnt give a 5 because I don't know what I need. I like it great\"]\n",
      "\n",
      "Num sentences: 2\n",
      "\n",
      "Transformed text: \n",
      "tensor([[1073,    2,   54,  307,   11, 1254,    7,    4,   86,  114,    6,    3,\n",
      "          139,   50,    2,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [4934,  240,    9,  189,  100,    4,   85,   10,   25,  155,   67,    4,\n",
      "          109,    2,    4,   60,    8,   68,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0]])\n",
      "\n",
      "Shape of transformed text: torch.Size([2, 50])\n"
     ]
    }
   ],
   "source": [
    "text = train_text[8:10]\n",
    "print(f\"Text: {text}\\n\")\n",
    "print(f\"Num sentences: {len(text)}\\n\")\n",
    "tt = transformText(text, max_len=50)\n",
    "print(f\"Transformed text: \\n{tt}\\n\")\n",
    "print(f\"Shape of transformed text: {tt.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. <a name=\"5\">Generating data batch and iterator</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Let's use the transformText() function and create the data loaders. Here, we use __max_len=100__ to consider the first 100 words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 100\n",
    "batch_size = 16\n",
    "\n",
    "# Pass transformed and padded data to dataset\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(\n",
    "    transformText(train_text, max_len), torch.tensor(train_label)\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "val_dataset = TensorDataset(transformText(val_text, max_len), torch.tensor(val_label))\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. <a name=\"6\">Using pre-trained GloVe Word Embeddings</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "In this example, we will use GloVe word vectors. `name='6B'` `dim=300` gives us 6 billion words/phrases vectors. Each word vector has 300 numbers in it. The following code shows how to get the word vectors and create an embedding matrix from them. We will connect our vocabulary indexes to the GloVe embedding with the `get_vecs_by_tokens()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:04:29.864398Z",
     "start_time": "2021-01-09T05:04:29.376025Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [02:41, 5.34MB/s]                               \n",
      "100%|█████████▉| 399999/400000 [00:39<00:00, 10171.69it/s]\n"
     ]
    }
   ],
   "source": [
    "glove = GloVe(name=\"6B\", dim=300)\n",
    "embedding_matrix = glove.get_vecs_by_tokens(vocab.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. <a name=\"7\">Setting Hyperparameters and Bulding the Network</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set our parameters like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:04:29.868989Z",
     "start_time": "2021-01-09T05:04:29.866241Z"
    }
   },
   "outputs": [],
   "source": [
    "# Size of the state vectors\n",
    "hidden_size = 8\n",
    "\n",
    "# General NN training parameters\n",
    "learning_rate = 0.001\n",
    "epochs = 25\n",
    "\n",
    "# Embedding vector and vocabulary sizes\n",
    "embed_size = 300  # glove.6B.300d.txt\n",
    "vocab_size = len(vocab.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to put our data into correct format before the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is made of these layers:\n",
    "* Embedding layer: This is where our words/tokens are mapped to word vectors.\n",
    "* RNN layer: We are using a simple RNN model. We stack 2 RNN layers in this example. More details about the RNN are available [here](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html).\n",
    "* Linear layer: A linear layer with a single neuron is used to output the `isPositive` prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:04:29.892791Z",
     "start_time": "2021-01-09T05:04:29.881808Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.RNN(\n",
    "            embed_size, hidden_size, num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size*max_len, 1)\n",
    "        self.act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        # Call RNN layer\n",
    "        outputs, _ = self.rnn(embeddings)\n",
    "        # Use the output of each time step\n",
    "        # Send it all together to the linear layer\n",
    "        outs = self.linear(outputs.reshape(outputs.shape[0], -1))\n",
    "        return self.act(outs)\n",
    "    \n",
    "model = Net(vocab_size, embed_size, hidden_size, num_layers=2)\n",
    "\n",
    "# Initialize the weights\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "    if type(m) == nn.RNN:\n",
    "        for param in m._flat_weights_names:\n",
    "            if \"weight\" in param:\n",
    "                nn.init.xavier_uniform_(m._parameters[param])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize this network. Then, we will need to make the embedding layer use our GloVe word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:04:29.902048Z",
     "start_time": "2021-01-09T05:04:29.899284Z"
    }
   },
   "outputs": [],
   "source": [
    "# We set the embedding layer's parameters from GloVe\n",
    "model.embedding.weight.data.copy_(embedding_matrix)\n",
    "# We won't change/train the embedding layer\n",
    "model.embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. <a name=\"8\">Training the Network</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Now, it is time to start our training. We define the loss function and training algorithm first. Then, training starts!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define the trainer and loss function below. \n",
    "\n",
    "__Binary cross-entropy loss__ is used as this is a binary classification problem.\n",
    "\n",
    "$$\n",
    "\\mathrm{BinaryCrossEntropyLoss} = -\\sum_{examples}{(y\\log(p) + (1 - y)\\log(1 - p))}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:04:29.906415Z",
     "start_time": "2021-01-09T05:04:29.903716Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setting our trainer\n",
    "trainer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# We will use Binary Cross-entropy loss\n",
    "# reduction=\"sum\" sums the losses for given output and target\n",
    "cross_ent_loss = nn.BCELoss(reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is time to start the training process. We will print the Binary cross-entropy loss loss after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:06:35.434926Z",
     "start_time": "2021-01-09T05:04:29.908071Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train_loss 0.5973615130545601. Val_loss 0.5182847450886454. Seconds 15.003392457962036\n",
      "Epoch 1. Train_loss 0.49063601183985905. Val_loss 0.47741369175059456. Seconds 14.428903102874756\n",
      "Epoch 2. Train_loss 0.46167316157666466. Val_loss 0.46435574889183046. Seconds 15.341912031173706\n",
      "Epoch 3. Train_loss 0.4447657239389798. Val_loss 0.45652317472866605. Seconds 14.96990418434143\n",
      "Epoch 4. Train_loss 0.43217158005824163. Val_loss 0.45046656757593156. Seconds 14.371657848358154\n",
      "Epoch 5. Train_loss 0.42222203342214465. Val_loss 0.4456157413550786. Seconds 15.766808986663818\n",
      "Epoch 6. Train_loss 0.4141492530679892. Val_loss 0.44175665663821356. Seconds 15.575850009918213\n",
      "Epoch 7. Train_loss 0.4074958306243495. Val_loss 0.438498275067125. Seconds 14.719719409942627\n",
      "Epoch 8. Train_loss 0.4019319280828275. Val_loss 0.43549691004412516. Seconds 16.524062156677246\n",
      "Epoch 9. Train_loss 0.39719844862700454. Val_loss 0.4326266840313162. Seconds 22.080443859100342\n",
      "Epoch 10. Train_loss 0.39311184639377256. Val_loss 0.4299386007445199. Seconds 20.785937309265137\n",
      "Epoch 11. Train_loss 0.38954701052771673. Val_loss 0.4275092681603772. Seconds 19.14646601676941\n",
      "Epoch 12. Train_loss 0.3864126470330216. Val_loss 0.42538480552179475. Seconds 21.623927116394043\n",
      "Epoch 13. Train_loss 0.38362750124836725. Val_loss 0.42358064755797387. Seconds 24.872910499572754\n",
      "Epoch 14. Train_loss 0.3811272051244501. Val_loss 0.4220568051508495. Seconds 34.78901672363281\n",
      "Epoch 15. Train_loss 0.37886213413661435. Val_loss 0.4208297077885696. Seconds 34.67844367027283\n",
      "Epoch 16. Train_loss 0.37678828201714964. Val_loss 0.41984825996415953. Seconds 32.659286975860596\n",
      "Epoch 17. Train_loss 0.37487320093408466. Val_loss 0.41907010020954266. Seconds 33.971673011779785\n",
      "Epoch 18. Train_loss 0.3730940790438936. Val_loss 0.41846422459397997. Seconds 33.53392958641052\n",
      "Epoch 19. Train_loss 0.371436028733613. Val_loss 0.41797527208924296. Seconds 35.20819926261902\n",
      "Epoch 20. Train_loss 0.36988816899912697. Val_loss 0.41754777029156687. Seconds 34.81882405281067\n",
      "Epoch 21. Train_loss 0.36843712274990387. Val_loss 0.4171598162821361. Seconds 33.91928768157959\n",
      "Epoch 22. Train_loss 0.3670702025034125. Val_loss 0.4168103491408484. Seconds 34.48068857192993\n",
      "Epoch 23. Train_loss 0.3657774910460862. Val_loss 0.4164978035432952. Seconds 33.66541242599487\n",
      "Epoch 24. Train_loss 0.36455101418471525. Val_loss 0.4162178333316531. Seconds 34.70782709121704\n"
     ]
    }
   ],
   "source": [
    "# Get the compute device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.apply(init_weights)\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    training_loss = 0\n",
    "    val_loss = 0\n",
    "    # Training loop, train the network\n",
    "    for data, target in train_loader:\n",
    "        trainer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        L = cross_ent_loss(output, target.unsqueeze(1))\n",
    "        training_loss += L.item()\n",
    "        L.backward()\n",
    "        trainer.step()\n",
    "\n",
    "    # Validate the network, no training (no weight update)\n",
    "    for data, target in val_loader:\n",
    "        val_predictions = model(data.to(device))\n",
    "        L = cross_ent_loss(val_predictions, target.to(device).unsqueeze(1))\n",
    "        val_loss += L.item()\n",
    "\n",
    "    # Let's take the average losses\n",
    "    training_loss = training_loss / len(train_label)\n",
    "    val_loss = val_loss / len(val_label)\n",
    "\n",
    "    end = time.time()\n",
    "    print(\n",
    "        f\"Epoch {epoch}. Train_loss {training_loss}. Val_loss {val_loss}. Seconds {end-start}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. <a name=\"9\">Test the classifier on the validation data</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Let's get the validation predictions. Earlier we made predictions on the validation set with this line: ```model(data.to(device))```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "val_predictions = []\n",
    "for data, target in val_loader:\n",
    "    val_preds = model(data.to(device))\n",
    "    val_predictions.extend(\n",
    "        [np.rint(val_pred)[0] for val_pred in val_preds.detach().cpu().numpy()]\n",
    "    )\n",
    "print(val_predictions[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix, classification report and accuracy score are printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1332  720]\n",
      " [ 337 3211]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.65      0.72      2052\n",
      "         1.0       0.82      0.91      0.86      3548\n",
      "\n",
      "    accuracy                           0.81      5600\n",
      "   macro avg       0.81      0.78      0.79      5600\n",
      "weighted avg       0.81      0.81      0.81      5600\n",
      "\n",
      "Accuracy (validation): 0.81125\n"
     ]
    }
   ],
   "source": [
    "# Use the fitted pipeline to make predictions on the validation dataset\n",
    "print(confusion_matrix(val_label, val_predictions))\n",
    "print(classification_report(val_label, val_predictions))\n",
    "print(\"Accuracy (validation):\", accuracy_score(val_label, val_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This score isn't a great improvement over the single layer network from yesterday (it can be even slightly worse than that result). RNNs usually require more data than regular neural networks in training. They also have additional hyperparameters to work with: __max_len, hidden_size, embed_size__ \n",
    "\n",
    "We will see some improved versions of RNNs namely Gated Recurrent Units and Long Sort-term Memory Networks tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. <a name=\"10\">Test the classifier on the unseen test data</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Let's get the test predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"../../data/examples/NLP-REVIEW-DATA-CLASSIFICATION-TEST.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = df_test[\"reviewText\"].fillna(value='').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(transformText(test_text, max_len)) #, torch.tensor(val_label))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "test_predictions = []\n",
    "for data, in test_loader:\n",
    "    test_preds = model(data.to(device))\n",
    "    test_predictions.extend(\n",
    "        [np.rint(test_pred)[0] for test_pred in test_preds.detach().cpu().numpy()]\n",
    "    )\n",
    "print(test_predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "result_df = pd.DataFrame()\n",
    "result_df[\"ID\"] = df_test[\"ID\"]\n",
    "result_df[\"isPositive\"] = test_predictions\n",
    "\n",
    "result_df.to_csv(\"result_day2_rnn.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. <a name=\"11\">Improvement ideas</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We can improve our model by\n",
    "* Changing hyper-parameters: Learning rate, batch size and hidden size\n",
    "* Increase the number of layers: num_layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
